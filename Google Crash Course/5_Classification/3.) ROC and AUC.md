### Receiver-operating characteristic curve (ROC)

#### What does the ROC curve tell you?

- A **perfect classifier** goes straight up to the top-left corner (TPR=1, FPR=0) — 100% recall and zero false positives.
- A **random classifier** gives a diagonal line from bottom-left to top-right (TPR = FPR).
- The closer your ROC curve is to the top-left, the better your model.
### AUC: Area Under the Curve

- It quantifies the overall ability of the model to discriminate between positive and negative classes.
- Ranges from **0 to 1**:
    
    - 1 means **perfect model**:-  This means there is a 100% probability that the model will                                                      correctly rank a randomly chosen positive example higher                                                     than a randomly chosen negative example.
    - 0.5 means **random guessing**
    - Less than 0.5 means worse than random (usually indicates a problem)


Sure ![[auc_1-0.png]]


Random Guess![[auc_0-5.png]]

The model with greater area under the curve is generally the better one.

### AUC and ROC for choosing model and threshold

The points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model

![[auc_abc.png]]

If false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable

