At a high level, ML problem framing consists of two distinct steps:

1. Determining whether ML is the right approach for solving a problem.
2. Framing the problem in ML terms.

### Understand the problem

To understand the problem, perform the following tasks:

- State the goal for the product you are developing or refactoring.
- Determine whether the goal is best solved using predictive ML, generative AI, or a non-ML solution.
- Verify you have the data required to train a model if you're using a predictive ML approach.

###  Predictive power

You can automate finding a feature's predictive power by using algorithms such as [Pearson correlation](https://wikipedia.org/wiki/Pearson_correlation_coefficient), [Adjusted mutual information (AMI)](https://wikipedia.org/wiki/Adjusted_mutual_information), and [Shapley value](https://wikipedia.org/wiki/Shapley_value#In_machine_learning), which provide a numerical assessment for analyzing the predictive power of a feature.

### Classification
A [**classification model**](https://developers.google.com/machine-learning/glossary#classification-model) predicts what category the input data belongs to
![[classification-model.png]]
### Regression
A [**regression model**](https://developers.google.com/machine-learning/glossary#regression-model) predicts a numerical value.
![[regression-model.png]]


![[regression-decision.png]]

[[2.) Loss]]
[[2.) Scenerio Probem]]
### Proxy labels

[**Proxy labels**](https://developers.google.com/machine-learning/glossary#proxy-labels) substitute for labels that aren't in the dataset. Proxy labels are necessary when you can't directly measure what you want to predict. In the video app, we can't directly measure whether or not a user will find a video useful
We use data like liked, shared, rewatched
### Generation

[**Distillation**](https://developers.google.com/machine-learning/glossary#distillation). To create a smaller version of a larger model, you generate a synthetic labeled dataset from the larger model that you use to train the smaller model.

 [**Fine-tuning**](https://developers.google.com/machine-learning/glossary#fine-tuning) or [**parameter-efficient tuning**](https://developers.google.com/machine-learning/glossary#parameter-efficient-tuning). To improve the performance of a model on a specific task, you need to further train the model on a dataset that contains examples of the type of output you want to produce.

[**Prompt engineering**](https://developers.google.com/machine-learning/glossary#prompt-engineering). To get the model to perform a specific task or produce output in a specific format, you tell the model the task you want it to do or explain how you want the output formatted.

Distillation and fine-tuning update the model's [**parameters**](https://developers.google.com/machine-learning/glossary#parameter). Prompt engineering doesn't update the model's parameters. Instead, prompt engineering helps the model learn how to produce a desired output from the context of the prompt.

Because of their deep knowledge of natural language, [**large language models (LLMs)**](https://developers.google.com/machine-learning/glossary#large-language-model) can frequently perform text classification tasks better than predictive ML trained for the specific task.



[[3.) Implementing a model]]