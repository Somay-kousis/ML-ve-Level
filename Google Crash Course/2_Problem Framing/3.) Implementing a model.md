Getting a full pipeline running for a complex model is harder than iterating on the model itself.
After setting up your data pipeline and implementing a simple model that uses a few features, you can iterate on creating a better model.


## Train your own model v/s Using an already trained model


Trained models only really work when the label and features match your dataset exactly

**Note:** If your solution is a generative AI model, you'll almost always fine-tune a [pre-trained model](https://developers.google.com/machine-learning/glossary#pre-trained-model)instead of training your own.

For information on trained models, see

- [Trained models from TensorFlow Hub](https://www.tensorflow.org/hub)
    
- [Trained models from Kaggle](https://www.kaggle.com/models)
    

## Monitoring

During problem framing, consider the monitoring and alerting infrastructure your ML solution needs.

### Model deployment

In some cases, a newly trained model might be worse than the model currently in production. If it is, you'll want to prevent it from being released into production and get an alert that your automated deployment has failed.

### Training-serving skew

If any of the incoming features used for inference have values that fall outside the distribution range of the data used in training, you'll want to be alerted because it's likely the model will make poor predictions. For example, if your model was trained to predict temperatures for equatorial cities at sea level, then your serving system should alert you of incoming data with latitudes and longitudes, and/or altitudes outside the range the model was trained on. Conversely, the serving system should alert you if the model is making predictions that are outside the distribution range that was seen during training.

### Inference server

If you're providing inferences through an RPC system, you'll want to monitor the RPC server itself and get an alert if it stops providing inferences.