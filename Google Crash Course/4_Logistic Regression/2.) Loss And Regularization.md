
###### Difference in between Linear and Logistic Regression

-  Logistic regression models use [**Log Loss**](https://developers.google.com/machine-learning/glossary#Log_Loss) as the loss function instead of [**squared loss**](https://developers.google.com/machine-learning/glossary#l2-loss).
- Applying [regularization](https://developers.google.com/machine-learning/crash-course/overfitting/regularization) is critical to prevent [**overfitting**](https://developers.google.com/machine-learning/glossary#overfitting).


#### Log Loss

$$
\text{Log Loss} = \sum_{(x,y)\in D} -y\log(y') - (1 - y)\log(1 - y')
$$

✅ x is the input
✅ y is the true output label 
✅ y′ is the predicted probability (from the model)


#### Regularization in logistic regression

A mechanism for penalizing model complexity during training. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in cases where the model has a large number of features. Consequently, most logistic regression models use one of the following two strategies to decrease model complexity:

- [L2 regularization](https://developers.google.com/machine-learning/crash-course/overfitting/regularization)
- [Early stopping](https://developers.google.com/machine-learning/crash-course/overfitting/regularization#early_stopping_an_alternative_to_complexity-based_regularization): Limiting the number of training steps to halt training while loss is still decreasing.




Imagine trying to draw a line through a noisy scatterplot:

- **No regularization:** You wiggle the line through every point, even outliers.
    
- **L2 regularization:** You force the line to be smoother—even if it doesn’t go through every point.
    
- **Early stopping:** You stop drawing the line before it gets too crazy trying to hit every point.
