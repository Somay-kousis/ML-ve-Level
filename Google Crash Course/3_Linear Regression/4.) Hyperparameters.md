Hyperparameters are values that you control; parameters are values that the model calculates during training. They control different aspects of training

- ### Learning rate:-
The goal is to pick a learning rate that's not too high nor too low so that the model converges quickly. Learning rate determines the magnitude of the changes:
- Too high = Never converges
- Too low = Long time to converge.


###### The difference between the old model parameters and the new model parameters is proportional to the slope of the loss function

Perfect LR

![[correct-lr.png]]


Small LR

![[small-lr.png]]


Large LR

![[high-lr.png]]



- ### Batch size:-
Number of [**examples**](https://developers.google.com/machine-learning/glossary#example) the model processes before updating its weights and bias.

##### Techniques:

- **Stochastic gradient descent (SGD)**:
One example comprising each batch is chosen at random but it contains a lot of noise 

If you have **100 training examples**, then **SGD** will:

- Randomly **pick 1 example** from the 100.
    
- Compute the gradient of the loss **only** with respect to that 1 example.
    
- Update the model parameters (like weight and bias).
    
- Then move on to the next iteration and repeat with **another random example** (or even the same one again, randomly).

![[noisy-gradient.png]]


- **Mini-batch stochastic gradient descent (mini-batch SGD)**:
Suppose you have **100 training examples**, and choose a **batch size of 10**:

- The dataset is **split into 10 mini-batches**, each with 10 examples.
    
- In each iteration, **one mini-batch** is used to:
    
    - Compute the average gradient over those 10 examples,
        
    - Update the model’s weights.
        
- After 10 updates, you've completed **one epoch**.

![[mini-batch-sgd.png]]


###  Epochs:-
It means that the model has processed every example
Given a training set with 1,000 examples and a mini-batch size of 100 examples, it will take the model 10 [**iterations**](https://developers.google.com/machine-learning/glossary#iteration) to complete one epoch.
More epochs produces a better model, but also takes more time to train.